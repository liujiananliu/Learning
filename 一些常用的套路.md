## 客户端连接数据库

1. **遵守JDBC**(支持SQL),按照JDBC的套路区连接数据库

1. 1. 获取Connection
   2. 准备sql，使用Connection预编译sql，获取PreparedStatement

1. 1. 1. 准备四个参数(不需要的可以不写)：

1. 1. 1. 1. url
         2. driver(省略，根据url自动加载驱动)
         3. user
         4. password

1. 1. 还可能会向预编译以后的sql中添加占位符参数
   2. 执行sql
   3. 如果是查询，遍历返回值

```java
public class FatClientDemo {
    public static void main(String[] args) throws SQLException {
        //准备url（FatClient）
        String url = "jdbc:phoenix:hadoop102:2181";
            //准备url（ThinClient）
            //String url = ThinClientUtil.getConnectionUrl("hadoop102", 8765);
        //获取Connection
        Connection connection = DriverManager.getConnection(url);
        //预编译sql
        PreparedStatement preparedStatement = connection.prepareStatement("select * from student1");
        //执行sql
        ResultSet resultSet = preparedStatement.executeQuery();
        //遍历返回值
        while (resultSet.next()){
            System.out.println(resultSet.getString(1) + ":" + resultSet.getString(2) + ":" +
                    resultSet.getString("addr"));
        }
        //关闭资源
        connection.close();
    }
}
```

1. **不遵守JDBC**(NoSQL-hbase,es,redis都不遵守JDBC)，按照一下套路：

1. 1. 新建一个客户端
   2. 可用户端与服务端建立连接
   3. 客户端发送命令到服务端执行
   4. 如果是读命令，接收服务端返回的结果
   5. 关闭客户端

## 构造一个对象

1. **new** A()
2. 把类的构造器私有，不允许new

1. 1. **建造者模式**：A a = new A.builder().build()
   2. **工厂模式**： A a = new  AFactory().getAInstance()

## 程序的入口和核心编程API

|                | 入口             | API                |
| -------------- | ---------------- | ------------------ |
| sparkcore      | SparkContext     | RDD                |
| sparksql       | SparkSession     | DataFrame, DataSet |
| sparkstreaming | StreamingContext | DStream            |

DStream: 离散化流，本质上是个流，由若干个离散的批组成

spark世界观：流是无限的批

DStream由无限个批次组成。可以对DStream定义一套计算逻辑，这个逻辑可以对DStream产生的每个批次都进行运算



## Spark编程套路

```scala
package com.atguigu.spark

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object WordCount {

    def main(args: Array[String]): Unit = {

        //1.创建SparkConf并设置App名称
        val conf = new SparkConf().setAppName("WC").setMaster("local[*]")

        //2.创建SparkContext，该对象是提交Spark App的入口
        val sc = new SparkContext(conf)

        //3.读取指定位置文件:hello atguigu atguigu
        val lineRdd: RDD[String] = sc.textFile("input")

        //4.读取的一行一行的数据分解成一个一个的单词（扁平化）(hello)(atguigu)(atguigu)
        val wordRdd: RDD[String] = lineRdd.flatMap(_.split(" "))

        //5. 将数据转换结构：(hello,1)(atguigu,1)(atguigu,1)
        val wordToOneRdd: RDD[(String, Int)] = wordRdd.map((_, 1))

        //6.将转换结构后的数据进行聚合处理 atguigu:1、1 =》1+1  (atguigu,2)
        val wordToSumRdd: RDD[(String, Int)] = wordToOneRdd.reduceByKey(_+_)

        //7.将统计结果采集到控制台打印
        wordToSumRdd.collect().foreach(println)

        //8.关闭连接
        sc.stop()
    }
}
```

## SparkStreaming编程套路

1. 创建入口: StreamingContext
2. 从StreamingContext中获取计算的流 DStream

1. 1. 读取文件中实时写入的数据：streamingContext.textFileStream("实时监控的目录")
   2. 读取某特TCP端口中实时发送的文本数据：streamingContext.socketTextStream()

1. 调用DStream 的API，进行各种transformation
2. 将最终的DStream进行输出
3. 启动App
4. 阻塞当前App，让他一直运行，直到发命令让他停止

```scala
val streamingContext = new StreamingContext("local[*]", "workCount", Seconds(5))
val kafkaParams = Map[String, Object](
  "bootstrap.servers" -> "hadoop102:9092",
  "key.deserializer" -> classOf[StringDeserializer],
  "value.deserializer" -> classOf[StringDeserializer],
  "group.id" -> "sz220409test",
  "auto.offset.reset" -> "latest",
  "enable.auto.commit" -> "true"
)

val topics = Array("topicD")
val ds: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream[String, String](
  streamingContext,
  PreferConsistent,
  Subscribe[String, String](topics, kafkaParams)
)

val ds1: DStream[(String, Int)] = ds.flatMap(record => record.value().split(" "))
.map(word => (word, 1))
.reduceByKey(_ + _)

ds1.print(1000)
streamingContext.start()
streamingContext.awaitTermination()
```