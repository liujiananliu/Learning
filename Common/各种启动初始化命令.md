# 各种启动/初始化命令

## hadoop

hdfs集群启动：`/opt/module/hadoop/sbin/start-dfs.sh`

hdfs集群关闭：`/opt/module/hadoop/sbin/stop-dfs.sh`

yarn集群启动：`/opt/module/hadoop/sbin/start-yarn.sh`

yarn集群关闭：`/opt/module/hadoop/sbin/stop-yarn.sh`

historyserver启动：`/opt/module/hadoop/bin/mapred --daemon start historyserver`

historyserver关闭：`/opt/module/hadoop/bin/mapred --daemon stop historyserver`

## zookeeper

zk节点启动：`/opt/module/zookeeper/bin/zkServer.sh start`

zk节点关闭：`/opt/module/zookeeper/bin/zkServer.sh stop`

## kafka

kafka节点启动： `/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties`

kafka节点关闭：`/opt/module/kafka/bin/kafka-server-stop.sh /opt/module/kafka/config/server.properties`



主题命令行操作：

```shell
#查看当前服务器中的所有topic
bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --list

#创建一个主题名为first的topic
bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --replication-factor 3 --partitions 1 --topic first

#查看Topic的详情
bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first

#修改分区数（注意：分区数只能增加，不能减少）
bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 3

#删除topic
bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic first
```

生产者命令行操作：

```shell
#生产消息
bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first
bin/kafka-console-producer.sh --topic atguigu --bootstrap-server  hadoop102:9092 
```

消费者命令行操作：

```shell
#消费消息
bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first

#从头消费
bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first
```





## flume

flume消费者启动（后台）： `nohup $FLUME_HOME/bin/flume-ng agent -n a1 -c $FLUME_HOME/conf/ -f $FLUME_HOME/job/flume-tailDir-kafka.conf -Dflume.root.logger=INFO,LOGFILE >$FLUME_HOME/logs/flume.log 2>&1 &`

flume消费者启动（窗口）：`$FLUME_HOME/bin/flume-ng agent -n a1 -c $FLUME_HOME/conf/ -f $FLUME_HOME/job/flume-tailDir-kafka.conf`

flume消费者关闭（后台）：`ps -ef | grep flume-tailDir-kafka | grep -v grep | awk '{print \$2}' | xargs -n1 kill -9`

flume消费者关闭（窗口）：`ctrl + c`



## datax

datax同步命令： `python /opt/module/datax/bin/datax.py -p"-Dexportdir=/warehouse/gmall/ads/ads_traffic_stats_by_channel" /opt/module/datax/job/export/gmall_report.ads_traffic_stats_by_channel.json`



## DophinScheduler

DolphinScheduler集群启动:  `/opt/module/dolphinscheduler/bin/start-all.sh`

DolphinScheduler单机模式启动:  `/opt/module/dolphinscheduler/bin/donphinscheduler-daemon.sh start standalone-server`



## conda

注：conda是一个开源的包、环境管理器，可以用于在同一个机器上安装不同Python版本的软件包及其依赖，并能够在不同的Python环境之间切换

激活superset环境:  `conda activate superset`

退出当前环境:  `conda deactivate`

删除superset环境:  `conda remove -n superset --all`



## superset

启动superset:  `gunicorn --workers 5 --timeout 120 --bind hadoop102:8787  "superset.app:create_app()" `

superset网页用户名和密码:  `admin 123321 `



## hbase

hbase服务单点启动:  `/opt/module/hbase/bin/hbase-daemon.sh start master`

hbase服务单点启动:  `/opt/module/hbase/bin/hbase-daemon.sh start regionserver`

hbase服务群启:  `/opt/module/hbase/bin/start-hbase.sh`

hbase服务群停:  `/opt/module/hbase/bin/stop-hbase.sh`

hbase shell客户端启动:  `/opt/module/hbase/bin/hbase shell`

hbase shell客户端关闭：`quit`

phoenix客户端启动:  `/opt/module/phoenix/bin/sqlline.py hadoop102,hadoop103,hadoop104:2181`

phoenix客户端关闭：`!quit`



## redis

redis服务端启动:  `/home/atguigu/bin/redis-server /home/atguigu/myredis/redis.conf`

redis服务端关闭：`ctrl + c`

redis客户端：`/home/atguigu/bin/redis-cli -h hadoop -p 6379`

redis客户端关闭：`quit`



## elasticsearch

elasticsearch单点启动:  `/opt/module/elasticsearch/bin/elasticsearch`

elasticsearch单点关闭:  `ps -ef | grep Elasticsearch | grep -v grep | awk  "{print \$2}" | xargs kill `



## kibana

elasticsearch客户端启动:  `/opt/module/kibana/bin/kibana`



## canal

canal服务启动：`/opt/module/canal/bin/startup.sh`

canal服务停止：`/opt/module/canal/bin/stop.sh`



## docker

docker服务启动：`sudo systemctl start docker`

查看docker服务是是否启动：`ps -ef | grep docker`





## flink

socket服务器开启：`nc -lk 9999 `

local-cluster模式启动本地集群：`bin/start-cluster.sh `

local-cluster模式关闭本地集群：`bin/stop-cluster.sh `

standalone模式启动本地集群：`bin/start-cluster.sh `

standalone模式关闭本地集群：`bin/stop-cluster.sh `

命令行提交Flink应用：`bin/flink run -m hadoop162:8081 -c com.atguigu.flink.java.chapter_2.Flink03_WC_UnBoundedStream ./flink-prepare-1.0-SNAPSHOT.jar `

yarn模式：

```shell
# yarn模式Session-Cluster模式执行无界流WordCount：
bin/flink run -d -t yarn-per-job -c com.atguigu.flink.java.chapter_2.Flink03_WC_UnBoundedStream ./flink-prepare-1.0-SNAPSHOT.jar

# yarn模式Per-Job-Cluster模式执行无界流WordCount：
bin/flink run -d -t yarn-per-job -c com.atguigu.flink.java.chapter_2.Flink03_WC_UnBoundedStream ./flink-prepare-1.0-SNAPSHOT.jar

# yarn模式Application Mode模式执行无界流WordCount：
bin/flink run-application -t yarn-application -c com.atguigu.flink.java.chapter_2.Flink03_WC_UnBoundedStream ./flink-prepare-1.0-SNAPSHOT.jar
```



kafka消费者（不读取未提交的数据）：` bin/kafka-console-consumer.sh --bootstrap-server hadoop162:9092 --topic s2 --isolation-level read_committed`
