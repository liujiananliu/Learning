# 客户端连接数据库

## 遵守JDBC(支持SQL)

**按照JDBC的套路连接数据库**

> 1. 获取Connection
>    准备四个参数(不需要的可以不写)：
>    1. url
>    2. driver(省略，根据url自动加载驱动)
>    3. user
>    4. password
> 2. 准备sql，使用Connection预编译sql，获取PreparedStatement
> 3. 还可能会向预编译以后的sql中添加占位符参数
> 4. 执行sql
> 5. 如果是查询，遍历返回值

```java
public class FatClientDemo {
    public static void main(String[] args) throws SQLException {
        //准备url（FatClient）
        String url = "jdbc:phoenix:hadoop102:2181";
            //准备url（ThinClient）
            //String url = ThinClientUtil.getConnectionUrl("hadoop102", 8765);
        //获取Connection
        Connection connection = DriverManager.getConnection(url);
        //预编译sql
        PreparedStatement preparedStatement = connection.prepareStatement("select * from student1");
        //执行sql
        ResultSet resultSet = preparedStatement.executeQuery();
        //遍历返回值
        while (resultSet.next()){
            System.out.println(resultSet.getString(1) + ":" + resultSet.getString(2) + ":" +
                    resultSet.getString("addr"));
        }
        //关闭资源
        connection.close();
    }
}
```

## 使用Druid从练级池中获取连接对象

> 1. 创建连接池对象
> 2. 连接池的初始化
>    1. 创建paramMap配置并赋值
>    2. 使用Druid连接池对象
> 3. 从连接池中获取连接对象

```scala
object JDBCUtil {
    // 创建连接池对象
    var dataSource:DataSource = init()

    // 连接池的初始化
    def init():DataSource = {

        val paramMap = new java.util.HashMap[String, String]()
        paramMap.put("driverClassName", PropertiesUtil.getValue("jdbc.driver.name"))
        paramMap.put("url", PropertiesUtil.getValue("jdbc.url"))
        paramMap.put("username", PropertiesUtil.getValue("jdbc.user"))
        paramMap.put("password", PropertiesUtil.getValue("jdbc.password"))
        paramMap.put("maxActive", PropertiesUtil.getValue("jdbc.datasource.size"))

        // 使用Druid连接池对象
        DruidDataSourceFactory.createDataSource(paramMap)
    }

    // 从连接池中获取连接对象
    def getConnection(): Connection = {
        dataSource.getConnection
    }
}
```







## 不遵守JDBC

> 1. 新建一个客户端
> 2. 可用客户端与服务建立连接
> 3. 客户端发送命令到服务端执行
> 4. 如果是读命令，接收服务端返回的结果
> 5. 关闭客户端

### 连接Redis

```scala
```



## Canal之CanalClient订阅数据的步骤

>1. 先创建一个客户端对象CanalConnector
>2. 使用客户端对象连接Canal server端
>3. 订阅表并拉取数据
>4. 解析订阅到的数据
>5. 将数据写入kafka

```java
```





# Socket

## socket输入

> 1 
>
>  

一个案例

```java
private static class mySocketSource implements SourceFunction<WaterSensor> {
        boolean isRunning = false;
        @Override
        public void run(SourceContext<WaterSensor> ctx) throws Exception {
            //实现一个从socket读取数据的source
            Socket socket = new Socket("hadoop162", 9999);
            //从网络中获取输入流
            InputStream is = socket.getInputStream();
            BufferedReader reader = new BufferedReader(new InputStreamReader(is, StandardCharsets.UTF_8));
            String line = reader.readLine();
            while (isRunning && line != null) {
                //对读到的一行做处理
                String[] data = line.split(",");
                ctx.collect((new WaterSensor(
                        data[0],
                        Long.valueOf(data[1]),
                        Integer.valueOf(data[2])
                )));
                line = reader.readLine();
            }
        }

        @Override
        public void cancel() {
            isRunning = false;
        }
    }
```





# Kafka

## Kafka生产者

> 1. 创建kafka生产者的配置对象
> 2. 给kafka配置对象添加配置信息
>    1. 设置bootstrap.servers（必须）
>    2. 设置key 和 value的序列化器（必须）
>    3. 设置batchsize
>    4. 设置linger.ms
>    5. 设置RecordAccumulator缓冲区大小
>    6. 设置compression.type压缩格式
>    7. 设置生产者是否是一个幂等生产者
> 3. 创建kafka生产者对象
> 4. 调用send方法，发送消息
> 5. 关闭资源

```java
public class CustomProducerParameters {
    public static void main(String[] args) throws InterruptedException {
        // 1. 创建kafka生产者的配置对象
        Properties properties = new Properties();

        // 2. 给kafka配置对象添加配置信息：bootstrap.servers
        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "hadoop102:9092");
        
        // key,value序列化（必须）：key.serializer，value.serializer
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");

        // batch.size：批次大小，默认16K
        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);

        // linger.ms：等待时间，默认0
        properties.put(ProducerConfig.LINGER_MS_CONFIG, 1);

        // RecordAccumulator：缓冲区大小，默认32M：buffer.memory
        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);

        // compression.type：压缩，默认none，可配置值gzip、snappy、lz4和zstd
properties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG,"snappy");

        // 3. 创建kafka生产者对象
        KafkaProducer<String, String> kafkaProducer = new KafkaProducer<String, String>(properties);

        // 4. 调用send方法,发送消息
        for (int i = 0; i < 5; i++) {
            kafkaProducer.send(new ProducerRecord<>("first","atguigu" + i));
        }

        // 5. 关闭资源
        kafkaProducer.close();
    }
}
```



# Json

```
fastjson:  阿里巴巴推出
           特点：   静态方法。

           核心类：  JSON
               {}字符串转java对象:   JSON.parseObject()
                       返回值:  JSONObject(Map)   或   T
               []字符串转java对象:   JSON.parseArray()
                       返回值:   JSONArray(List)   或  List[T]


               java对象转字符串:   JSON.toJSONString()


gson:      google推出
          特点：   实例方法

          核心类：  Gson

               java对象转字符串:   Gson.toJSON()
```

**字符串还原为java对象，建议用fastjson**

**java对象转json字符串，推荐用gson**

# Spark

## Spark

> 1. 创建SparkContext
> 2. 通过上下对象获取rdd
> 3. 对rdd做各种转换算子
> 4. 执行一个行动算子
> 5. 关闭连接

```scala
object WordCount {
    def main(args: Array[String]): Unit = {
        //创建SparkConf并设置App名称
        val conf = new SparkConf().setAppName("WC").setMaster("local[*]")
        //1.创建SparkContext，该对象是提交Spark App的入口
        val sc = new SparkContext(conf)
        //2.通过SparkContext获取rdd,这里是读取指定位置文件
        val lineRdd: RDD[String] = sc.textFile("input")
        //3.对rdd做各种转换算子
  	    val resultRdd: RDD[(String, Int)] = lineRdd.flatMap(_.split(" ")).map(x => (x, 1)).reduceByKey(_ + _)
        //4.执行一个行动算子
        resultRdd.collect().foreach(println)
        //5.关闭连接
        sc.stop()
    }
}
```



## SparkStreaming编程的套路

> 1. 创建入口StreamingContext
> 2. 使用StreamingContext获取DStream(数据流)
>    1.   读取文件中实时写入的数据：streamingContext.textFileStream("实时监控的目录")
>    2.   读取某特TCP端口中实时发送的文本数据：streamingContext.socketTextStream()
> 3. 使用DStream的API，进行各种transformation
> 4. 将最终的DStreaming进行输出
> 5. 启动App
> 6. 阻塞当前App，让他一直运行，直到发命令让他停止

```scala
object HelloWorld {
  def main(args: Array[String]): Unit = {
    //①创建入口StreamingContext
    val streamingContext = new StreamingContext("local[*]", "testAppName", Seconds(5))
    //②使用StreamingContext获取DStream（数据流）
    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> "hadoop102:9092",
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "testGroupId",
      "auto.offset.reset" -> "latest",
      "enable.auto.commit" -> "true"
    )
    val topics = Array("topicA")
    val ds = KafkaUtils.createDirectStream[String, String](
      streamingContext,
      PreferConsistent,
      Subscribe[String, String](topics, kafkaParams)
    )
    //③使用DStream的API，进行各种transformation
    val ds1: DStream[(String, Int)] = ds.flatMap(_.value().split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
    //④将最终的DStreaming进行输出
    ds1.print(1000)
    //⑤启动App
    streamingContext.start()
    //⑥阻塞当前App，让他一直运行，直到发命令让他停止
    streamingContext.awaitTermination()
  }
}
```



# Flink

## wordCount简单案例

> 1. 创建执行环境
> 2. 通过环境从数据源（source）获取一个流
> 3. 对流做各种转换
> 4. 把流输出
> 5. 执行执行环境

```java
public class Flink02_WC_UnBounded_Practice {
    public static void main(String[] args) {
        //创建执行环境
        Configuration conf = new Configuration();
        conf.setInteger("rest.port", 2000);//设置网页端口(可不设置)
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(conf);
        env.setParallelism(1);
//        env.disableOperatorChaining();
        //通过环境从数据源获一个流
        DataStreamSource<String> source = env.socketTextStream("hadoop162", 9999);
        //对流进行各种转换
        SingleOutputStreamOperator<String> wordStream = source.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public void flatMap(String value, Collector<String> out) throws Exception {
                for (String word : value.split(" ")) {
                    out.collect(word);
                }
            }
        });
        SingleOutputStreamOperator<Tuple2<String, Long>> wordOneStream = wordStream.map(new MapFunction<String, Tuple2<String, Long>>() {
            @Override
            public Tuple2<String, Long> map(String value) throws Exception {
                return Tuple2.of(value, 1L);
            }
        });
        KeyedStream<Tuple2<String, Long>, String> keyedStream = wordOneStream.keyBy((KeySelector<Tuple2<String, Long>, String>) value -> value.f0);
        SingleOutputStreamOperator<Tuple2<String, Long>> result = keyedStream.sum(1);
        //把流输出
        result.print();
        //执行 执行环境
        try {
            env.execute();
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }
}
```

**精简版写法：（使用Lambda表达式和方法引用）**

```java
public static void main(String[] args) {
    //1.创建执行环境
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    //2.通过执行环境从数据源获取一个流
    env
        .socketTextStream("hadoop162", 9999)
        //3.对流做各种转换
        .flatMap((String line, Collector<String> out) -> {
            //另一种写法： flatMap((FlatMapFunction<String, String>) (value, out) -> {
            Arrays.stream(line.split(" ")).forEach(out::collect);
            //另一种写法： Arrays.stream(line.split(" ")).forEach(word -> out.collect(word));
        })
        .returns(Types.STRING)
        .map(word -> Tuple2.of(word, 1L))
        .returns(Types.TUPLE(Types.STRING, Types.LONG))
        .keyBy(t -> t.f0)
        .sum(1)
        //4.把流输出
        .print();
    //5.执行执行环境
    try {
        env.execute();
    } catch (Exception e) {
        throw new RuntimeException(e);
    }
}
```

## Broadcast State

> 1. 读取一个数据流
> 2. 读取一个配置流
> 3. 把配置流做成一个广播流
> 4. 让数据流去connect广播流
> 5. 分别处理广播流中的数据和数据流中的数据
>    1. 广播流中的数据存入到广播状态，状态自动广播到每个并行度中
>    2. 处理数据流，从广播状态读取配置信息，来根据配置信息决定代码的处理方式

```java
public class Flink01_Operator_BroadCastState {
    public static void main(String[] args) {
        Configuration conf = new Configuration();
        conf.setInteger("rest.port", 2000);
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(conf);
        env.setParallelism(2);

        env.enableCheckpointing(3000);

        //1. 读取一个数据流
        DataStreamSource<String> dataStream = env.socketTextStream("hadoop162", 8888);
        //2. 读取一个配置流
        DataStreamSource<String> confStream = env.socketTextStream("hadoop162", 9999);
        //3. 把配置流做成一个广播流
        MapStateDescriptor<String, String> bcStateDesc = new MapStateDescriptor<>("bcState", String.class, String.class);
        BroadcastStream<String> bcStream = confStream.broadcast(bcStateDesc);
        //4. 让数据流去connect广播流
        BroadcastConnectedStream<String, String> dataConfStream = dataStream.connect(bcStream);
        //5. 分别处理广播流中的数据和数据流中的数据
        dataConfStream
                .process(new BroadcastProcessFunction<String, String, String>() {
                    //5.2 处理数据流，从广播状态读取配置信息，来根据配置信息决定代码的处理方式
                    //数据流中的数据，每来一条执行一次
                    @Override
                    public void processElement(String data, BroadcastProcessFunction<String, String, String>.ReadOnlyContext ctx, Collector<String> out) throws Exception {
                        System.out.println("Flink01_Operator_BroadCastState.processElement");
                        ReadOnlyBroadcastState<String, String> state = ctx.getBroadcastState(bcStateDesc);
                        String aSwitch = state.get("aSwitch");
                        if ("1".equals(aSwitch)) {
                            out.collect("使用1号逻辑");
                        } else if ("2".equals(aSwitch)) {
                            out.collect("使用2号逻辑");
                        } else {
                            out.collect("使用默认号逻辑");
                        }
                    }

                    //5.1 广播流中的数据存入到广播状态，状态自动广播到每个并行度中
                    //配置流中的数据每来一条，每个并行度执行一次
                    @Override
                    public void processBroadcastElement(String value, BroadcastProcessFunction<String, String, String>.Context ctx, Collector<String> out) throws Exception {
                        System.out.println("Flink01_Operator_BroadCastState.processBroadcastElement");
                        BroadcastState<String, String> state = ctx.getBroadcastState(bcStateDesc);
                        state.put("aSwitch",value);
                    }
                })
                .print();

        try {
            env.execute();
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }
}
```



